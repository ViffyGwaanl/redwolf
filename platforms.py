"""
Â§öÂπ≥Âè∞APIÊúçÂä°Ê®°Âùó
ÊîØÊåÅËá™ÂÆö‰πâOpenAI API„ÄÅOpenRouter„ÄÅOllama„ÄÅLMStudioÁ≠âÂπ≥Âè∞
"""

import httpx
import json
import asyncio
from typing import Dict, List, Any, Optional, AsyncGenerator
from dataclasses import dataclass
from enum import Enum
import logging

# ÈÖçÁΩÆÊó•Âøó
import os
DEBUG_MODE = os.getenv('DEBUG_MODE', 'false').lower() == 'true'

logging.basicConfig(level=logging.DEBUG if DEBUG_MODE else logging.INFO)
logger = logging.getLogger(__name__)

def debug_print(*args, **kwargs):
    """Áªü‰∏ÄÁöÑDEBUGËæìÂá∫ÂáΩÊï∞ÔºåÂè™Âú®DEBUG_MODEÂêØÁî®Êó∂ËæìÂá∫"""
    if DEBUG_MODE:
        print(*args, **kwargs)

class PlatformType(Enum):
    """Âπ≥Âè∞Á±ªÂûãÊûö‰∏æ"""
    CUSTOM_OPENAI = "custom_openai"  # Ëá™ÂÆö‰πâOpenAI API
    OPENROUTER = "openrouter"
    OLLAMA = "ollama"
    LMSTUDIO = "lmstudio"

@dataclass
class PlatformConfig:
    """Âπ≥Âè∞ÈÖçÁΩÆ"""
    platform_type: PlatformType
    api_key: str = ""
    base_url: str = ""
    enabled: bool = True
    timeout: int = 30

@dataclass
class ModelInfo:
    """Ê®°Âûã‰ø°ÊÅØ"""
    id: str
    name: str
    platform: PlatformType
    enabled: bool = True
    description: str = ""

class PlatformClient:
    """Âπ≥Âè∞ÂÆ¢Êà∑Á´ØÂü∫Á±ª"""
    
    def __init__(self, config: PlatformConfig):
        self.config = config
        self.client = None
    
    async def get_models(self) -> List[ModelInfo]:
        """Ëé∑ÂèñÂèØÁî®Ê®°ÂûãÂàóË°®"""
        raise NotImplementedError
    
    async def chat_completion(
        self, 
        model: str, 
        messages: List[Dict[str, Any]], 
        stream: bool = False,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """ËÅäÂ§©Ë°•ÂÖ®Êé•Âè£"""
        raise NotImplementedError
    
    async def test_connection(self) -> bool:
        """ÊµãËØïËøûÊé•"""
        try:
            models = await self.get_models()
            return len(models) > 0
        except Exception as e:
            logger.error(f"Platform {self.config.platform_type} connection test failed: {e}")
            return False

class CustomOpenAIClient(PlatformClient):
    """Ëá™ÂÆö‰πâOpenAI APIÂÆ¢Êà∑Á´Ø"""
    
    def __init__(self, config: PlatformConfig):
        super().__init__(config)
        self.base_url = config.base_url or "https://api.openai.com"
    
    async def get_models(self) -> List[ModelInfo]:
        """Ëé∑ÂèñËá™ÂÆö‰πâOpenAI APIÊ®°ÂûãÂàóË°®"""
        logger.info("üîç [CustomOpenAI] ÂºÄÂßãËé∑ÂèñÊ®°ÂûãÂàóË°®...")
        
        if not self.config.api_key:
            logger.warning("‚ö†Ô∏è [CustomOpenAI] API KeyÊú™ÈÖçÁΩÆÔºåË∑≥ËøáËé∑ÂèñÊ®°Âûã")
            return []
        
        try:
            logger.info(f"üåê [CustomOpenAI] ËØ∑Ê±ÇURL: {self.base_url}/v1/models")
            async with httpx.AsyncClient(timeout=self.config.timeout) as client:
                response = await client.get(
                    f"{self.base_url}/v1/models",
                    headers={
                        "Authorization": f"Bearer {self.config.api_key}",
                        "Content-Type": "application/json"
                    }
                )
                
                logger.info(f"üì° [CustomOpenAI] APIÂìçÂ∫îÁä∂ÊÄÅ: {response.status_code}")
                
                if response.status_code == 200:
                    data = response.json()
                    models = []
                    
                    logger.info(f"üìã [CustomOpenAI] ÂìçÂ∫îÊï∞ÊçÆ: {json.dumps(data, indent=2, ensure_ascii=False)}")
                    
                    # Ëß£ÊûêÊ†áÂáÜOpenAI APIÊ®°ÂûãÂàóË°®Ê†ºÂºè
                    if "data" in data:
                        for model in data["data"]:
                            model_info = ModelInfo(
                                id=model.get("id", ""),
                                name=model.get("id", ""),
                                platform=PlatformType.CUSTOM_OPENAI,
                                description=model.get("description", f"ÂàõÂª∫Êó∂Èó¥: {model.get('created', 'Unknown')}")
                            )
                            models.append(model_info)
                    else:
                        # Â¶ÇÊûúAPIËøîÂõûÊ†ºÂºè‰∏çÂåπÈÖçÔºåÊ∑ªÂä†‰∏Ä‰∫õÂ∏∏ËßÅÁöÑÈªòËÆ§Ê®°Âûã
                        logger.info("‚ö†Ô∏è [CustomOpenAI] APIÂìçÂ∫îÊ†ºÂºè‰∏çÂåπÈÖçÔºå‰ΩøÁî®ÈªòËÆ§Ê®°ÂûãÂàóË°®")
                        default_models = [
                            {"id": "gpt-4", "name": "gpt-4", "description": "GPT-4 Ê®°Âûã"},
                            {"id": "gpt-4-turbo", "name": "gpt-4-turbo", "description": "GPT-4 Turbo Ê®°Âûã"},
                            {"id": "gpt-3.5-turbo", "name": "gpt-3.5-turbo", "description": "GPT-3.5 Turbo Ê®°Âûã"},
                            {"id": "claude-3-opus", "name": "claude-3-opus", "description": "Claude 3 Opus Ê®°Âûã"},
                            {"id": "claude-3-sonnet", "name": "claude-3-sonnet", "description": "Claude 3 Sonnet Ê®°Âûã"},
                            {"id": "claude-3-haiku", "name": "claude-3-haiku", "description": "Claude 3 Haiku Ê®°Âûã"},
                        ]
                        
                        for model in default_models:
                            model_info = ModelInfo(
                                id=model["id"],
                                name=model["name"],
                                platform=PlatformType.CUSTOM_OPENAI,
                                description=model["description"]
                            )
                            models.append(model_info)
            
                    
                    logger.info(f"‚úÖ [CustomOpenAI] ÊàêÂäüËé∑Âèñ {len(models)} ‰∏™Ê®°Âûã")
                    return models
                else:
                    logger.error(f"‚ùå [CustomOpenAI] APIÈîôËØØ: {response.status_code} - {response.text}")
                    return []
                    
        except Exception as e:
            logger.error(f"‚ùå [CustomOpenAI] Ëé∑ÂèñÊ®°ÂûãÂ§±Ë¥•: {e}")
            return []
    
    async def chat_completion(
        self, 
        model: str, 
        messages: List[Dict[str, Any]], 
        stream: bool = False,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """Ëá™ÂÆö‰πâOpenAI APIËÅäÂ§©Ë°•ÂÖ®"""
        if not self.config.api_key:
            yield json.dumps({"error": "API key not configured"})
            return
        
        url = f"{self.base_url}/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.config.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": model,
            "messages": messages,
            "stream": stream,
            **kwargs
        }
        
        try:
            async with httpx.AsyncClient(timeout=self.config.timeout) as client:
                if stream:
                    async with client.stream(
                        "POST", url, headers=headers, json=payload
                    ) as response:
                        if response.status_code == 200:
                            async for line in response.aiter_lines():
                                if line.strip():
                                    if line.startswith("data: "):
                                        data = line[6:]
                                        if data.strip() == "[DONE]":
                                            break
                                        yield data
                                    else:
                                        yield line
                        else:
                            error_msg = await response.aread()
                            yield json.dumps({"error": f"API error: {response.status_code} - {error_msg.decode()}"})
                else:
                    response = await client.post(url, headers=headers, json=payload)
                    if response.status_code == 200:
                        yield response.text
                    else:
                        yield json.dumps({"error": f"API error: {response.status_code} - {response.text}"})
                        
        except Exception as e:
            logger.error(f"CustomOpenAI chat completion error: {e}")
            yield json.dumps({"error": f"Request failed: {str(e)}"})

class OpenRouterClient(PlatformClient):
    """OpenRouterÂÆ¢Êà∑Á´Ø"""
    
    def __init__(self, config: PlatformConfig):
        super().__init__(config)
        self.base_url = "https://openrouter.ai/api/v1"
    
    async def get_models(self) -> List[ModelInfo]:
        """Ëé∑ÂèñOpenRouterÊ®°ÂûãÂàóË°®"""
        if not self.config.api_key:
            return []
        
        try:
            async with httpx.AsyncClient(timeout=self.config.timeout) as client:
                response = await client.get(
                    f"{self.base_url}/models",
                    headers={
                        "Authorization": f"Bearer {self.config.api_key}",
                        "Content-Type": "application/json"
                    }
                )
                
                if response.status_code == 200:
                    data = response.json()
                    models = []
                    
                    if "data" in data:
                        for model in data["data"]:
                            models.append(ModelInfo(
                                id=model.get("id", ""),
                                name=model.get("name", model.get("id", "")),
                                platform=PlatformType.OPENROUTER,
                                description=model.get("description", "")
                            ))
                    
                    return models
                else:
                    logger.error(f"OpenRouter API error: {response.status_code} - {response.text}")
                    return []
                    
        except Exception as e:
            logger.error(f"Failed to get OpenRouter models: {e}")
            return []
    
    async def chat_completion(
        self, 
        model: str, 
        messages: List[Dict[str, Any]], 
        stream: bool = False,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """OpenRouterËÅäÂ§©Ë°•ÂÖ®"""
        if not self.config.api_key:
            yield json.dumps({"error": "API key not configured"})
            return
        
        url = f"{self.base_url}/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.config.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": model,
            "messages": messages,
            "stream": stream,
            **kwargs
        }
        
        try:
            async with httpx.AsyncClient(timeout=self.config.timeout) as client:
                if stream:
                    async with client.stream(
                        "POST", url, headers=headers, json=payload
                    ) as response:
                        if response.status_code == 200:
                            async for line in response.aiter_lines():
                                if line.strip():
                                    # Áõ¥Êé• yield ÂéüÂßãË°åÔºåËÆ©ËΩ¨Êç¢Âô®Â§ÑÁêÜÊ†ºÂºè
                                    yield line
                        else:
                            error_msg = await response.aread()
                            yield json.dumps({"error": f"API error: {response.status_code} - {error_msg.decode()}"})
                else:
                    response = await client.post(url, headers=headers, json=payload)
                    if response.status_code == 200:
                        yield response.text
                    else:
                        yield json.dumps({"error": f"API error: {response.status_code} - {response.text}"})
                        
        except Exception as e:
            logger.error(f"OpenRouter chat completion error: {e}")
            yield json.dumps({"error": f"Request failed: {str(e)}"})

class OllamaClient(PlatformClient):
    """OllamaÂÆ¢Êà∑Á´Ø"""
    
    def __init__(self, config: PlatformConfig):
        super().__init__(config)
        self.base_url = config.base_url or "http://localhost:11434"
    
    async def get_models(self) -> List[ModelInfo]:
        """Ëé∑ÂèñOllamaÊ®°ÂûãÂàóË°®"""
        logger.info("üîç [Ollama] ÂºÄÂßãËé∑ÂèñÊ®°ÂûãÂàóË°®...")
        logger.info(f"üåê [Ollama] ËØ∑Ê±ÇURL: {self.base_url}/api/tags")
        
        try:
            async with httpx.AsyncClient(timeout=self.config.timeout) as client:
                response = await client.get(f"{self.base_url}/api/tags")
                
                logger.info(f"üì° [Ollama] APIÂìçÂ∫îÁä∂ÊÄÅ: {response.status_code}")
                
                if response.status_code == 200:
                    data = response.json()
                    models = []
                    
                    logger.info(f"üìã [Ollama] ÂìçÂ∫îÊï∞ÊçÆ: {json.dumps(data, indent=2, ensure_ascii=False)}")
                    
                    if "models" in data:
                        for model in data["models"]:
                            model_info = ModelInfo(
                                id=model.get("name", ""),
                                name=model.get("name", ""),
                                platform=PlatformType.OLLAMA,
                                description=f"Size: {model.get('size', 'Unknown')}"
                            )
                            models.append(model_info)
            
                    
                    logger.info(f"‚úÖ [Ollama] ÊàêÂäüËé∑Âèñ {len(models)} ‰∏™Ê®°Âûã")
                    return models
                else:
                    logger.error(f"‚ùå [Ollama] APIÈîôËØØ: {response.status_code} - {response.text}")
                    return []
                    
        except Exception as e:
            logger.error(f"‚ùå [Ollama] Ëé∑ÂèñÊ®°ÂûãÂ§±Ë¥•: {e}")
            return []
    
    async def chat_completion(
        self, 
        model: str, 
        messages: List[Dict[str, Any]], 
        stream: bool = True,  # OllamaÈªòËÆ§‰ΩøÁî®ÊµÅÂºè
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """OllamaËÅäÂ§©Ë°•ÂÖ®"""
        url = f"{self.base_url}/api/chat"
        
        payload = {
            "model": model,
            "messages": messages,
            "stream": stream
        }
        
        try:
            async with httpx.AsyncClient(timeout=self.config.timeout) as client:
                if stream:
                    async with client.stream(
                        "POST", url, json=payload
                    ) as response:
                        if response.status_code == 200:
                            async for line in response.aiter_lines():
                                if line.strip():
                                    try:
                                        data = json.loads(line)
                                        # ËΩ¨Êç¢OllamaÊ†ºÂºèÂà∞OpenAIÊ†ºÂºè
                                        openai_chunk = self._convert_ollama_to_openai(data)
                                        yield json.dumps(openai_chunk)
                                        
                                        if data.get("done", False):
                                            break
                                    except json.JSONDecodeError:
                                        continue
                        else:
                            error_msg = await response.aread()
                            yield json.dumps({"error": f"API error: {response.status_code} - {error_msg.decode()}"})
                else:
                    # ÈùûÊµÅÂºèÊ®°ÂºèÈúÄË¶ÅÊâãÂä®Êî∂ÈõÜÊâÄÊúâÂìçÂ∫î
                    full_response = ""
                    async with client.stream("POST", url, json=payload) as response:
                        async for line in response.aiter_lines():
                            if line.strip():
                                try:
                                    data = json.loads(line)
                                    if "message" in data and "content" in data["message"]:
                                        full_response += data["message"]["content"]
                                    if data.get("done", False):
                                        break
                                except json.JSONDecodeError:
                                    continue
                    
                    openai_response = {
                        "id": "chatcmpl-ollama",
                        "object": "chat.completion",
                        "created": int(asyncio.get_event_loop().time()),
                        "model": model,
                        "choices": [{
                            "index": 0,
                            "message": {
                                "role": "assistant",
                                "content": full_response
                            },
                            "finish_reason": "stop"
                        }]
                    }
                    yield json.dumps(openai_response)
                        
        except Exception as e:
            logger.error(f"Ollama chat completion error: {e}")
            yield json.dumps({"error": f"Request failed: {str(e)}"})
    
    def _convert_ollama_to_openai(self, ollama_data: Dict[str, Any]) -> Dict[str, Any]:
        """Â∞ÜOllamaÂìçÂ∫îÊ†ºÂºèËΩ¨Êç¢‰∏∫OpenAIÊ†ºÂºè"""
        content = ""
        if "message" in ollama_data and "content" in ollama_data["message"]:
            content = ollama_data["message"]["content"]
        
        return {
            "id": "chatcmpl-ollama",
            "object": "chat.completion.chunk",
            "created": int(asyncio.get_event_loop().time()),
            "model": ollama_data.get("model", "unknown"),
            "choices": [{
                "index": 0,
                "delta": {
                    "content": content
                } if content else {},
                "finish_reason": "stop" if ollama_data.get("done", False) else None
            }]
        }

class LMStudioClient(PlatformClient):
    """LMStudioÂÆ¢Êà∑Á´Ø"""
    
    def __init__(self, config: PlatformConfig):
        super().__init__(config)
        self.base_url = config.base_url or "http://localhost:1234"
    
    async def get_models(self) -> List[ModelInfo]:
        """Ëé∑ÂèñLMStudioÊ®°ÂûãÂàóË°®"""
        try:
            async with httpx.AsyncClient(timeout=self.config.timeout) as client:
                response = await client.get(f"{self.base_url}/v1/models")
                
                if response.status_code == 200:
                    data = response.json()
                    models = []
                    
                    if "data" in data:
                        for model in data["data"]:
                            models.append(ModelInfo(
                                id=model.get("id", ""),
                                name=model.get("id", ""),
                                platform=PlatformType.LMSTUDIO,
                                description="LMStudio local model"
                            ))
                    
                    return models
                else:
                    logger.error(f"LMStudio API error: {response.status_code} - {response.text}")
                    return []
                    
        except Exception as e:
            logger.error(f"Failed to get LMStudio models: {e}")
            return []
    
    async def chat_completion(
        self, 
        model: str, 
        messages: List[Dict[str, Any]], 
        stream: bool = False,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """LMStudioËÅäÂ§©Ë°•ÂÖ®"""
        url = f"{self.base_url}/v1/chat/completions"
        headers = {
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": model,
            "messages": messages,
            "stream": stream,
            **kwargs
        }
        
        try:
            async with httpx.AsyncClient(timeout=self.config.timeout) as client:
                if stream:
                    async with client.stream(
                        "POST", url, headers=headers, json=payload
                    ) as response:
                        if response.status_code == 200:
                            async for line in response.aiter_lines():
                                if line.strip():
                                    if line.startswith("data: "):
                                        data = line[6:]
                                        if data.strip() == "[DONE]":
                                            break
                                        yield data
                                    else:
                                        yield line
                        else:
                            error_msg = await response.aread()
                            yield json.dumps({"error": f"API error: {response.status_code} - {error_msg.decode()}"})
                else:
                    response = await client.post(url, headers=headers, json=payload)
                    if response.status_code == 200:
                        yield response.text
                    else:
                        yield json.dumps({"error": f"API error: {response.status_code} - {response.text}"})
                        
        except Exception as e:
            logger.error(f"LMStudio chat completion error: {e}")
            yield json.dumps({"error": f"Request failed: {str(e)}"})

class PlatformManager:
    """Âπ≥Âè∞ÁÆ°ÁêÜÂô®"""
    
    def __init__(self):
        self.platforms: Dict[PlatformType, PlatformClient] = {}
    
    def add_platform(self, config: PlatformConfig):
        """Ê∑ªÂä†Âπ≥Âè∞"""
        if config.platform_type == PlatformType.CUSTOM_OPENAI:
            client = CustomOpenAIClient(config)
        elif config.platform_type == PlatformType.OPENROUTER:
            client = OpenRouterClient(config)
        elif config.platform_type == PlatformType.OLLAMA:
            client = OllamaClient(config)
        elif config.platform_type == PlatformType.LMSTUDIO:
            client = LMStudioClient(config)
        else:
            raise ValueError(f"Unsupported platform type: {config.platform_type}")
        
        self.platforms[config.platform_type] = client
    
    def get_platform(self, platform_type: PlatformType) -> Optional[PlatformClient]:
        """Ëé∑ÂèñÂπ≥Âè∞ÂÆ¢Êà∑Á´Ø"""
        return self.platforms.get(platform_type)
    
    async def get_all_models(self) -> List[ModelInfo]:
        """Ëé∑ÂèñÊâÄÊúâÂπ≥Âè∞ÁöÑÊ®°ÂûãÂàóË°®"""
        logger.info("üöÄ [PlatformManager] ÂºÄÂßãËé∑ÂèñÊâÄÊúâÂπ≥Âè∞Ê®°ÂûãÂàóË°®...")
        
        all_models = []
        for platform_type, platform in self.platforms.items():
            try:
                logger.info(f"üìû [PlatformManager] Ë∞ÉÁî® {platform_type.value} Âπ≥Âè∞...")
                models = await platform.get_models()
                logger.info(f"üì¶ [PlatformManager] {platform_type.value} ËøîÂõû {len(models)} ‰∏™Ê®°Âûã")
                all_models.extend(models)
            except Exception as e:
                logger.error(f"‚ùå [PlatformManager] {platform_type.value} Âπ≥Âè∞Ëé∑ÂèñÊ®°ÂûãÂ§±Ë¥•: {e}")
        
        logger.info(f"üéØ [PlatformManager] ÊÄªÂÖ±Ëé∑ÂèñÂà∞ {len(all_models)} ‰∏™Ê®°Âûã")
        return all_models
    
    async def test_all_connections(self) -> Dict[PlatformType, bool]:
        """ÊµãËØïÊâÄÊúâÂπ≥Âè∞ËøûÊé•"""
        results = {}
        for platform_type, client in self.platforms.items():
            results[platform_type] = await client.test_connection()
        
        return results