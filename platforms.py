"""
å¤šå¹³å°APIæœåŠ¡æ¨¡å—
æ”¯æŒè‡ªå®šä¹‰OpenAI APIã€OpenRouterã€Ollamaã€LMStudioç­‰å¹³å°
"""

import httpx
import json
import asyncio
from typing import Dict, List, Any, Optional, AsyncGenerator
from dataclasses import dataclass
from enum import Enum
import logging

# é…ç½®æ—¥å¿—
import os
DEBUG_MODE = os.getenv('DEBUG_MODE', 'false').lower() == 'true'

logging.basicConfig(level=logging.DEBUG if DEBUG_MODE else logging.INFO)
logger = logging.getLogger(__name__)

def debug_print(*args, **kwargs):
    """ç»Ÿä¸€çš„DEBUGè¾“å‡ºå‡½æ•°ï¼Œåªåœ¨DEBUG_MODEå¯ç”¨æ—¶è¾“å‡º"""
    if DEBUG_MODE:
        print(*args, **kwargs)

class PlatformType(Enum):
    """å¹³å°ç±»å‹æšä¸¾"""
    CUSTOM_OPENAI = "custom_openai"  # è‡ªå®šä¹‰OpenAI API
    OPENROUTER = "openrouter"
    OLLAMA = "ollama"
    LMSTUDIO = "lmstudio"

@dataclass
class PlatformConfig:
    """å¹³å°é…ç½®"""
    platform_type: PlatformType
    api_key: str = ""
    base_url: str = ""
    enabled: bool = True
    timeout: int = 30

@dataclass
class ModelInfo:
    """æ¨¡å‹ä¿¡æ¯"""
    id: str
    name: str
    platform: PlatformType
    enabled: bool = True
    description: str = ""

class PlatformClient:
    """å¹³å°å®¢æˆ·ç«¯åŸºç±»"""
    
    def __init__(self, config: PlatformConfig):
        self.config = config
        self.client = None
    
    async def get_models(self) -> List[ModelInfo]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        raise NotImplementedError
    
    async def chat_completion(
        self, 
        model: str, 
        messages: List[Dict[str, Any]], 
        stream: bool = False,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """èŠå¤©è¡¥å…¨æ¥å£"""
        raise NotImplementedError
    
    async def test_connection(self) -> bool:
        """æµ‹è¯•è¿æ¥"""
        try:
            models = await self.get_models()
            return len(models) > 0
        except Exception as e:
            logger.error(f"Platform {self.config.platform_type} connection test failed: {e}")
            return False

class CustomOpenAIClient(PlatformClient):
    """è‡ªå®šä¹‰OpenAI APIå®¢æˆ·ç«¯"""
    
    def __init__(self, config: PlatformConfig):
        super().__init__(config)
        self.base_url = config.base_url or "https://api.openai.com"
    
    async def get_models(self) -> List[ModelInfo]:
        """è·å–è‡ªå®šä¹‰OpenAI APIæ¨¡å‹åˆ—è¡¨"""
        logger.info("ğŸ” [CustomOpenAI] å¼€å§‹è·å–æ¨¡å‹åˆ—è¡¨...")
        
        if not self.config.api_key:
            logger.warning("âš ï¸ [CustomOpenAI] API Keyæœªé…ç½®ï¼Œè·³è¿‡è·å–æ¨¡å‹")
            return []
        
        try:
            logger.info(f"ğŸŒ [CustomOpenAI] è¯·æ±‚URL: {self.base_url}/v1/models")
            async with httpx.AsyncClient(timeout=self.config.timeout) as client:
                response = await client.get(
                    f"{self.base_url}/v1/models",
                    headers={
                        "Authorization": f"Bearer {self.config.api_key}",
                        "Content-Type": "application/json"
                    }
                )
                
                logger.info(f"ğŸ“¡ [CustomOpenAI] APIå“åº”çŠ¶æ€: {response.status_code}")
                
                if response.status_code == 200:
                    data = response.json()
                    models = []
                    
                    logger.info(f"ğŸ“‹ [CustomOpenAI] å“åº”æ•°æ®: {json.dumps(data, indent=2, ensure_ascii=False)}")
                    
                    # è§£ææ ‡å‡†OpenAI APIæ¨¡å‹åˆ—è¡¨æ ¼å¼
                    if "data" in data:
                        for model in data["data"]:
                            model_info = ModelInfo(
                                id=model.get("id", ""),
                                name=model.get("id", ""),
                                platform=PlatformType.CUSTOM_OPENAI,
                                description=model.get("description", f"åˆ›å»ºæ—¶é—´: {model.get('created', 'Unknown')}")
                            )
                            models.append(model_info)
                    else:
                        # å¦‚æœAPIè¿”å›æ ¼å¼ä¸åŒ¹é…ï¼Œæ·»åŠ ä¸€äº›å¸¸è§çš„é»˜è®¤æ¨¡å‹
                        logger.info("âš ï¸ [CustomOpenAI] APIå“åº”æ ¼å¼ä¸åŒ¹é…ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹åˆ—è¡¨")
                        default_models = [
                            {"id": "gpt-4", "name": "gpt-4", "description": "GPT-4 æ¨¡å‹"},
                            {"id": "gpt-4-turbo", "name": "gpt-4-turbo", "description": "GPT-4 Turbo æ¨¡å‹"},
                            {"id": "gpt-3.5-turbo", "name": "gpt-3.5-turbo", "description": "GPT-3.5 Turbo æ¨¡å‹"},
                            {"id": "claude-3-opus", "name": "claude-3-opus", "description": "Claude 3 Opus æ¨¡å‹"},
                            {"id": "claude-3-sonnet", "name": "claude-3-sonnet", "description": "Claude 3 Sonnet æ¨¡å‹"},
                            {"id": "claude-3-haiku", "name": "claude-3-haiku", "description": "Claude 3 Haiku æ¨¡å‹"},
                        ]
                        
                        for model in default_models:
                            model_info = ModelInfo(
                                id=model["id"],
                                name=model["name"],
                                platform=PlatformType.CUSTOM_OPENAI,
                                description=model["description"]
                            )
                            models.append(model_info)
            
                    
                    logger.info(f"âœ… [CustomOpenAI] æˆåŠŸè·å– {len(models)} ä¸ªæ¨¡å‹")
                    return models
                else:
                    logger.error(f"âŒ [CustomOpenAI] APIé”™è¯¯: {response.status_code} - {response.text}")
                    return []
                    
        except Exception as e:
            logger.error(f"âŒ [CustomOpenAI] è·å–æ¨¡å‹å¤±è´¥: {e}")
            return []
    
    async def chat_completion(
        self, 
        model: str, 
        messages: List[Dict[str, Any]], 
        stream: bool = False,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """è‡ªå®šä¹‰OpenAI APIèŠå¤©è¡¥å…¨"""
        if not self.config.api_key:
            yield json.dumps({"error": "API key not configured"})
            return
        
        url = f"{self.base_url}/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.config.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": model,
            "messages": messages,
            "stream": stream,
            **kwargs
        }
        
        try:
            async with httpx.AsyncClient(timeout=self.config.timeout) as client:
                if stream:
                    async with client.stream(
                        "POST", url, headers=headers, json=payload
                    ) as response:
                        if response.status_code == 200:
                            async for line in response.aiter_lines():
                                if line.strip():
                                    if line.startswith("data: "):
                                        data = line[6:]
                                        if data.strip() == "[DONE]":
                                            break
                                        yield data
                                    else:
                                        yield line
                        else:
                            error_msg = await response.aread()
                            yield json.dumps({"error": f"API error: {response.status_code} - {error_msg.decode()}"})
                else:
                    response = await client.post(url, headers=headers, json=payload)
                    if response.status_code == 200:
                        yield response.text
                    else:
                        yield json.dumps({"error": f"API error: {response.status_code} - {response.text}"})
                        
        except Exception as e:
            logger.error(f"CustomOpenAI chat completion error: {e}")
            yield json.dumps({"error": f"Request failed: {str(e)}"})

class OpenRouterClient(PlatformClient):
    """OpenRouterå®¢æˆ·ç«¯"""
    
    def __init__(self, config: PlatformConfig):
        super().__init__(config)
        self.base_url = "https://openrouter.ai/api/v1"
    
    async def get_models(self) -> List[ModelInfo]:
        """è·å–OpenRouteræ¨¡å‹åˆ—è¡¨"""
        if not self.config.api_key:
            return []
        
        try:
            async with httpx.AsyncClient(timeout=self.config.timeout) as client:
                response = await client.get(
                    f"{self.base_url}/models",
                    headers={
                        "Authorization": f"Bearer {self.config.api_key}",
                        "Content-Type": "application/json"
                    }
                )
                
                if response.status_code == 200:
                    data = response.json()
                    models = []
                    
                    if "data" in data:
                        for model in data["data"]:
                            models.append(ModelInfo(
                                id=model.get("id", ""),
                                name=model.get("name", model.get("id", "")),
                                platform=PlatformType.OPENROUTER,
                                description=model.get("description", "")
                            ))
                    
                    return models
                else:
                    logger.error(f"OpenRouter API error: {response.status_code} - {response.text}")
                    return []
                    
        except Exception as e:
            logger.error(f"Failed to get OpenRouter models: {e}")
            return []
    
    async def chat_completion(
        self, 
        model: str, 
        messages: List[Dict[str, Any]], 
        stream: bool = False,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """OpenRouterèŠå¤©è¡¥å…¨"""
        if not self.config.api_key:
            yield json.dumps({"error": "API key not configured"})
            return
        
        url = f"{self.base_url}/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.config.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": model,
            "messages": messages,
            "stream": stream,
            **kwargs
        }
        
        try:
            async with httpx.AsyncClient(timeout=self.config.timeout) as client:
                if stream:
                    async with client.stream(
                        "POST", url, headers=headers, json=payload
                    ) as response:
                        if response.status_code == 200:
                            async for line in response.aiter_lines():
                                if line.strip():
                                    # ç›´æ¥ yield åŸå§‹è¡Œï¼Œè®©è½¬æ¢å™¨å¤„ç†æ ¼å¼
                                    yield line
                        else:
                            error_msg = await response.aread()
                            yield json.dumps({"error": f"API error: {response.status_code} - {error_msg.decode()}"})
                else:
                    response = await client.post(url, headers=headers, json=payload)
                    if response.status_code == 200:
                        yield response.text
                    else:
                        yield json.dumps({"error": f"API error: {response.status_code} - {response.text}"})
                        
        except Exception as e:
            logger.error(f"OpenRouter chat completion error: {e}")
            yield json.dumps({"error": f"Request failed: {str(e)}"})

class OllamaClient(PlatformClient):
    """Ollamaå®¢æˆ·ç«¯"""
    
    def __init__(self, config: PlatformConfig):
        super().__init__(config)
        self.base_url = config.base_url or "http://localhost:11434"
    
    async def get_models(self) -> List[ModelInfo]:
        """è·å–Ollamaæ¨¡å‹åˆ—è¡¨"""
        logger.info("ğŸ” [Ollama] å¼€å§‹è·å–æ¨¡å‹åˆ—è¡¨...")
        logger.info(f"ğŸŒ [Ollama] è¯·æ±‚URL: {self.base_url}/api/tags")
        
        try:
            async with httpx.AsyncClient(timeout=self.config.timeout) as client:
                response = await client.get(f"{self.base_url}/api/tags")
                
                logger.info(f"ğŸ“¡ [Ollama] APIå“åº”çŠ¶æ€: {response.status_code}")
                
                if response.status_code == 200:
                    data = response.json()
                    models = []
                    
                    logger.info(f"ğŸ“‹ [Ollama] å“åº”æ•°æ®: {json.dumps(data, indent=2, ensure_ascii=False)}")
                    
                    if "models" in data:
                        for model in data["models"]:
                            model_info = ModelInfo(
                                id=model.get("name", ""),
                                name=model.get("name", ""),
                                platform=PlatformType.OLLAMA,
                                description=f"Size: {model.get('size', 'Unknown')}"
                            )
                            models.append(model_info)
            
                    
                    logger.info(f"âœ… [Ollama] æˆåŠŸè·å– {len(models)} ä¸ªæ¨¡å‹")
                    return models
                else:
                    logger.error(f"âŒ [Ollama] APIé”™è¯¯: {response.status_code} - {response.text}")
                    return []
                    
        except Exception as e:
            logger.error(f"âŒ [Ollama] è·å–æ¨¡å‹å¤±è´¥: {e}")
            return []
    
    async def chat_completion(
        self, 
        model: str, 
        messages: List[Dict[str, Any]], 
        stream: bool = True,  # Ollamaé»˜è®¤ä½¿ç”¨æµå¼
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """OllamaèŠå¤©è¡¥å…¨"""
        url = f"{self.base_url}/api/chat"
        
        payload = {
            "model": model,
            "messages": messages,
            "stream": stream
        }
        
        try:
            async with httpx.AsyncClient(timeout=self.config.timeout) as client:
                if stream:
                    async with client.stream(
                        "POST", url, json=payload
                    ) as response:
                        if response.status_code == 200:
                            async for line in response.aiter_lines():
                                if line.strip():
                                    try:
                                        data = json.loads(line)
                                        # è½¬æ¢Ollamaæ ¼å¼åˆ°OpenAIæ ¼å¼
                                        openai_chunk = self._convert_ollama_to_openai(data)
                                        yield json.dumps(openai_chunk)
                                        
                                        if data.get("done", False):
                                            break
                                    except json.JSONDecodeError:
                                        continue
                        else:
                            error_msg = await response.aread()
                            yield json.dumps({"error": f"API error: {response.status_code} - {error_msg.decode()}"})
                else:
                    # éæµå¼æ¨¡å¼éœ€è¦æ‰‹åŠ¨æ”¶é›†æ‰€æœ‰å“åº”
                    full_response = ""
                    async with client.stream("POST", url, json=payload) as response:
                        async for line in response.aiter_lines():
                            if line.strip():
                                try:
                                    data = json.loads(line)
                                    if "message" in data and "content" in data["message"]:
                                        full_response += data["message"]["content"]
                                    if data.get("done", False):
                                        break
                                except json.JSONDecodeError:
                                    continue
                    
                    openai_response = {
                        "id": "chatcmpl-ollama",
                        "object": "chat.completion",
                        "created": int(asyncio.get_event_loop().time()),
                        "model": model,
                        "choices": [{
                            "index": 0,
                            "message": {
                                "role": "assistant",
                                "content": full_response
                            },
                            "finish_reason": "stop"
                        }]
                    }
                    yield json.dumps(openai_response)
                        
        except Exception as e:
            logger.error(f"Ollama chat completion error: {e}")
            yield json.dumps({"error": f"Request failed: {str(e)}"})
    
    def _convert_ollama_to_openai(self, ollama_data: Dict[str, Any]) -> Dict[str, Any]:
        """å°†Ollamaå“åº”æ ¼å¼è½¬æ¢ä¸ºOpenAIæ ¼å¼"""
        content = ""
        if "message" in ollama_data and "content" in ollama_data["message"]:
            content = ollama_data["message"]["content"]
        
        return {
            "id": "chatcmpl-ollama",
            "object": "chat.completion.chunk",
            "created": int(asyncio.get_event_loop().time()),
            "model": ollama_data.get("model", "unknown"),
            "choices": [{
                "index": 0,
                "delta": {
                    "content": content
                } if content else {},
                "finish_reason": "stop" if ollama_data.get("done", False) else None
            }]
        }

class LMStudioClient(PlatformClient):
    """LMStudioå®¢æˆ·ç«¯"""
    
    def __init__(self, config: PlatformConfig):
        super().__init__(config)
        self.base_url = config.base_url or "http://localhost:1234"
    
    async def get_models(self) -> List[ModelInfo]:
        """è·å–LMStudioæ¨¡å‹åˆ—è¡¨"""
        try:
            async with httpx.AsyncClient(timeout=self.config.timeout) as client:
                response = await client.get(f"{self.base_url}/v1/models")
                
                if response.status_code == 200:
                    data = response.json()
                    models = []
                    
                    if "data" in data:
                        for model in data["data"]:
                            models.append(ModelInfo(
                                id=model.get("id", ""),
                                name=model.get("id", ""),
                                platform=PlatformType.LMSTUDIO,
                                description="LMStudio local model"
                            ))
                    
                    return models
                else:
                    logger.error(f"LMStudio API error: {response.status_code} - {response.text}")
                    return []
                    
        except Exception as e:
            logger.error(f"Failed to get LMStudio models: {e}")
            return []
    
    async def chat_completion(
        self, 
        model: str, 
        messages: List[Dict[str, Any]], 
        stream: bool = False,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """LMStudioèŠå¤©è¡¥å…¨"""
        url = f"{self.base_url}/v1/chat/completions"
        headers = {
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": model,
            "messages": messages,
            "stream": stream,
            **kwargs
        }
        
        try:
            async with httpx.AsyncClient(timeout=self.config.timeout) as client:
                if stream:
                    async with client.stream(
                        "POST", url, headers=headers, json=payload
                    ) as response:
                        if response.status_code == 200:
                            async for line in response.aiter_lines():
                                if line.strip():
                                    if line.startswith("data: "):
                                        data = line[6:]
                                        if data.strip() == "[DONE]":
                                            break
                                        yield data
                                    else:
                                        yield line
                        else:
                            error_msg = await response.aread()
                            yield json.dumps({"error": f"API error: {response.status_code} - {error_msg.decode()}"})
                else:
                    response = await client.post(url, headers=headers, json=payload)
                    if response.status_code == 200:
                        yield response.text
                    else:
                        yield json.dumps({"error": f"API error: {response.status_code} - {response.text}"})
                        
        except Exception as e:
            logger.error(f"LMStudio chat completion error: {e}")
            yield json.dumps({"error": f"Request failed: {str(e)}"})

class PlatformManager:
    """å¹³å°ç®¡ç†å™¨"""
    
    def __init__(self):
        self.platforms: Dict[PlatformType, PlatformClient] = {}
    
    def add_platform(self, config: PlatformConfig):
        """æ·»åŠ å¹³å°"""
        if config.platform_type == PlatformType.CUSTOM_OPENAI:
            client = CustomOpenAIClient(config)
        elif config.platform_type == PlatformType.OPENROUTER:
            client = OpenRouterClient(config)
        elif config.platform_type == PlatformType.OLLAMA:
            client = OllamaClient(config)
        elif config.platform_type == PlatformType.LMSTUDIO:
            client = LMStudioClient(config)
        else:
            raise ValueError(f"Unsupported platform type: {config.platform_type}")
        
        self.platforms[config.platform_type] = client
    
    def get_platform(self, platform_type: PlatformType) -> Optional[PlatformClient]:
        """è·å–å¹³å°å®¢æˆ·ç«¯"""
        return self.platforms.get(platform_type)
    
    async def get_all_models(self) -> List[ModelInfo]:
        """è·å–æ‰€æœ‰å¹³å°çš„æ¨¡å‹åˆ—è¡¨"""
        logger.info("ğŸš€ [PlatformManager] å¼€å§‹è·å–æ‰€æœ‰å¹³å°æ¨¡å‹åˆ—è¡¨...")
        
        all_models = []
        for platform_type, platform in self.platforms.items():
            try:
                logger.info(f"ğŸ“ [PlatformManager] è°ƒç”¨ {platform_type.value} å¹³å°...")
                models = await platform.get_models()
                logger.info(f"ğŸ“¦ [PlatformManager] {platform_type.value} è¿”å› {len(models)} ä¸ªæ¨¡å‹")
                all_models.extend(models)
            except Exception as e:
                logger.error(f"âŒ [PlatformManager] {platform_type.value} å¹³å°è·å–æ¨¡å‹å¤±è´¥: {e}")
        
        logger.info(f"ğŸ¯ [PlatformManager] æ€»å…±è·å–åˆ° {len(all_models)} ä¸ªæ¨¡å‹")
        return all_models
    
    async def test_all_connections(self) -> Dict[PlatformType, bool]:
        """æµ‹è¯•æ‰€æœ‰å¹³å°è¿æ¥"""
        results = {}
        for platform_type, client in self.platforms.items():
            results[platform_type] = await client.test_connection()
        
        return results